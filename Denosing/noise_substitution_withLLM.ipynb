{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## noisy한 데이터를 ascii 코드 기준으로 텍스트의 0.x 이상 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, '../data')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, 'outputs')\n",
    "\n",
    "# labeled_data_path = os.path.join(DATA_DIR, \"noised_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ID                              text  target  ascii_ratio  \\\n",
      "0  ynat-v1_train_00000  정^ ^파^ 미사^ ^^^ 이용기간 ^^ 단^ ^분종^^보       4     0.406250   \n",
      "1  ynat-v1_train_00001       ^찰^국^^^^ 로^^한^^ 회장 ^ ^^^^송^       3     0.592593   \n",
      "2  ynat-v1_train_00002            ^ 김정^ 자주통일 새^^^열^나가야^보       2     0.318182   \n",
      "3  ynat-v1_train_00003     갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩       5     0.103448   \n",
      "4  ynat-v1_train_00004      ^^美대선^앞두고 ^^^^단 발^ ^비해 감시 강화       6     0.321429   \n",
      "\n",
      "   is_noisy  \n",
      "0         1  \n",
      "1         1  \n",
      "2         1  \n",
      "3         0  \n",
      "4         1  \n"
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "threshold = 0.2\n",
    "df = pd.read_csv(DATA_DIR+f\"/train_modified_t{threshold}_ver2.csv\")\n",
    "df_list.append(df)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = df_list[0]['text'][3]\n",
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b26a3cc4d68429787ade5327412c8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/57.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86adb46e32c144ad8e19bf431c027af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743d997ca6474667a1526e14bede160e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/462 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7e418651c84bf7bfd9c2089110c7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/913 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955d0c26392640dd92cf13d8e68e3e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/22.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "248e1186b5b844979d9b056edb230044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea7fe052c484644b53367e7e2be1b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e026bebc3824157ac6539f2fff23ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7b538e145942e4aad9e4fb13417e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dc67f45a9d4f0ead8774ebef13232f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93ab62b7b50469882155a1af8e1dd57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# 모델과 토크나이저 로드\n",
    "model_name = 'Saxo/Linkbricks-Horizon-AI-Korean-Pro-8B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# 노이즈가 있는 뉴스 기사 제목\n",
    "noisy_title = \"세계K인무역협회 올S2 무역인 1천b55w 배출한\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name :  Saxo/Linkbricks-Horizon-AI-Korean-Pro-8B\n",
      "original text :  세계K인무역협회 올S2 무역인 1천b55w 배출한\n",
      "(step_prompt_result) :  '세계K인무역협회 올해 무역인 1천명 배출한'\n",
      "\n",
      "설명:\n",
      "1. 'S2'와 '1천b55w'는 무작위로 삽입된 영문\n",
      "------------\n",
      "(few_shot_result) :  '세계K인무역협회 올해 S2 무\n",
      "------------\n",
      "(back_translation) :  '세계K인무역협회 올S2 무역인 1천명 배출한' (노이즈 제거 및 문맥 유지)\n",
      "\n",
      "번역 과정:\n",
      "1단계 - 영어 번역: 'World K Trade\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 작성\n",
    "prompt1 = f\"\"\"당신은 전문적인 한국어 텍스트 정제 AI입니다. 다음은 노이즈가 포함된 한국어 뉴스 기사의 제목입니다. 이 제목에서 노이즈를 제거하고 원래의 자연스러운 뉴스 제목으로 복원해주세요.\n",
    "\n",
    "노이즈가 포함된 제목: '{noisy_title}'\n",
    "\n",
    "복원 시 다음 지침을 따라주세요:\n",
    "1. 무작위로 삽입된 영문자와 숫자를 제거하세요.\n",
    "2. 특수문자를 적절히 처리하세요.\n",
    "3. 줄임말이나 약어는 가능한 원래 형태로 복원하세요.\n",
    "4. 문맥을 고려하여 누락된 단어나 조사를 추가하세요.\n",
    "5. 제목의 전체적인 의미를 유지하면서 자연스러운 한국어 문장으로 만드세요.\n",
    "\n",
    "복원된 제목:\"\"\"\n",
    "\n",
    "prompt2 = f\"\"\"당신은 전문적인 한국어 텍스트 정제 AI입니다. 다음은 노이즈가 포함된 한국어 뉴스 기사의 제목입니다. 이 제목에서 노이즈를 제거하고 원래의 자연스러운 뉴스 제목으로 복원해주세요.\n",
    "\n",
    "다음은 노이즈가 제거된 예시입니다:\n",
    "\n",
    "1. 노이즈가 포함된 제목: '게시판 KISA 박민정 책임연구원 APTLD 이사 선출'\n",
    "   복원된 제목: 'KISA 박민정 책임연구원, APTLD 이사 선출'\n",
    "\n",
    "2. 노이즈가 포함된 제목: 'UEFA 챔스리그 4강 마드리드 더비 성사레알 vs 아'\n",
    "   복원된 제목: 'UEFA 챔스리그 4강, 마드리드 더비 성사... 레알 vs 아틀레티코'\n",
    "\n",
    "3. 노이즈가 포함된 제목: '본t카 애스턴마틴 Sp라 국내 4P500대g 생산'\n",
    "   복원된 제목: '본드카 애스턴마틴 스포츠카, 국내 450대 생산'\n",
    "\n",
    "이제 다음 제목의 노이즈를 제거해주세요:\n",
    "\n",
    "노이즈가 포함된 제목: '{noisy_title}'\n",
    "\n",
    "복원된 제목:\"\"\"\n",
    "\n",
    "prompt3 = f\"\"\"당신은 다국어 능력을 갖춘 전문 번역 AI입니다. 다음은 노이즈가 포함된 한국어 뉴스 기사의 제목입니다. 이 제목을 정제하기 위해 다음 단계를 수행해주세요:\n",
    "\n",
    "1. 노이즈가 포함된 한국어 제목을 영어로 번역하세요. 이 때, 노이즈로 보이는 부분은 최대한 문맥을 고려하여 자연스럽게 해석하세요.\n",
    "2. 번역된 영어 제목을 다시 한국어로 번역하세요. 이 과정에서 원래 제목의 의미를 최대한 보존하면서 자연스러운 한국어 뉴스 제목을 만들어주세요.\n",
    "\n",
    "참고: 다음은 이 과정의 예시입니다.\n",
    "\n",
    "예시 1:\n",
    "노이즈가 포함된 한국어 제목: '게시판 KISA 박민정 책임연구원 APTLD 이사 선출'\n",
    "1단계 - 영어 번역: 'KISA Senior Researcher Park Min-jung Elected as APTLD Director'\n",
    "2단계 - 한국어 재번역: 'KISA 박민정 책임연구원, APTLD 이사로 선출'\n",
    "\n",
    "예시 2:\n",
    "노이즈가 포함된 한국어 제목: 'UEFA 챔스리그 4강 마드리드 더비 성사레알 vs 아'\n",
    "1단계 - 영어 번역: 'UEFA Champions League Semifinals: Madrid Derby Confirmed, Real vs Atletico'\n",
    "2단계 - 한국어 재번역: 'UEFA 챔피언스리그 4강: 마드리드 더비 성사, 레알 vs 아틀레티코'\n",
    "\n",
    "이제 주어진 제목에 대해 위 과정을 수행해주세요.\n",
    "노이즈가 포함된 제목: '{noisy_title}'\n",
    "\n",
    "복원된 제목:\"\"\"\n",
    "\n",
    "# 토크나이즈 및 모델 입력 생성\n",
    "input1 = tokenizer(prompt1, return_tensors=\"pt\").to(model.device)\n",
    "input2 = tokenizer(prompt2, return_tensors=\"pt\").to(model.device)\n",
    "input3 = tokenizer(prompt3, return_tensors=\"pt\").to(model.device)\n",
    "# 모델을 사용하여 텍스트 생성\n",
    "with torch.no_grad():\n",
    "    outputs1 = model.generate(\n",
    "        **input1,\n",
    "        max_new_tokens=50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    outputs2 = model.generate(\n",
    "        **input2,\n",
    "        max_new_tokens=50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    outputs3 = model.generate(\n",
    "        **input3,\n",
    "        max_new_tokens=50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "# 생성된 텍스트 디코딩\n",
    "generated_text1 = tokenizer.decode(outputs1[0], skip_special_tokens=True)\n",
    "generated_text2 = tokenizer.decode(outputs2[0], skip_special_tokens=True)\n",
    "generated_text3 = tokenizer.decode(outputs3[0], skip_special_tokens=True)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"model_name : \", model_name)\n",
    "print(\"original text : \", noisy_title)\n",
    "print(\"(step_prompt_result) : \", generated_text1.split(\"복원된 제목:\")[-1].strip())\n",
    "print(\"------------\")\n",
    "print(\"(few_shot_result) : \",generated_text2.split(\"복원된 제목:\")[-1].strip())\n",
    "print(\"------------\")\n",
    "print(\"(back_translation) : \",generated_text3.split(\"복원된 제목:\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>ascii_ratio</th>\n",
       "      <th>is_noisy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ynat-v1_train_00000</td>\n",
       "      <td>정   파  미사      이용기간    단   분종  보</td>\n",
       "      <td>4</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ynat-v1_train_00001</td>\n",
       "      <td>찰 국     로  한   회장       송</td>\n",
       "      <td>3</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ynat-v1_train_00002</td>\n",
       "      <td>김정  자주통일 새   열 나가야 보</td>\n",
       "      <td>2</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ynat-v1_train_00003</td>\n",
       "      <td>갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩</td>\n",
       "      <td>5</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ynat-v1_train_00004</td>\n",
       "      <td>美대선 앞두고     단 발   비해 감시 강화</td>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2795</th>\n",
       "      <td>ynat-v1_train_02795</td>\n",
       "      <td>트럼프 폭스뉴스 앵커들 충성도 점수매겨…10점만점에 12점도</td>\n",
       "      <td>6</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>ynat-v1_train_02796</td>\n",
       "      <td>삼성 갤럭시S9 정식 출시 첫 주말 이통시장 잠잠</td>\n",
       "      <td>2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2797</th>\n",
       "      <td>ynat-v1_train_02797</td>\n",
       "      <td>텔레그램 한  등 亞서  시간 다운… 버 정    종  보</td>\n",
       "      <td>4</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2798</th>\n",
       "      <td>ynat-v1_train_02798</td>\n",
       "      <td>인터뷰 류현진 친구에게 안타 맞는 것 싫어해…승부는 냉정</td>\n",
       "      <td>1</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799</th>\n",
       "      <td>ynat-v1_train_02799</td>\n",
       "      <td>지능정보사회 대비 국가 종합대책 마련</td>\n",
       "      <td>4</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2800 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       ID                               text  target  \\\n",
       "0     ynat-v1_train_00000   정   파  미사      이용기간    단   분종  보       4   \n",
       "1     ynat-v1_train_00001         찰 국     로  한   회장       송        3   \n",
       "2     ynat-v1_train_00002               김정  자주통일 새   열 나가야 보       2   \n",
       "3     ynat-v1_train_00003      갤노트8 주말 27만대 개통…시장은 불법 보조금 얼룩       5   \n",
       "4     ynat-v1_train_00004         美대선 앞두고     단 발   비해 감시 강화       6   \n",
       "...                   ...                                ...     ...   \n",
       "2795  ynat-v1_train_02795  트럼프 폭스뉴스 앵커들 충성도 점수매겨…10점만점에 12점도       6   \n",
       "2796  ynat-v1_train_02796        삼성 갤럭시S9 정식 출시 첫 주말 이통시장 잠잠       2   \n",
       "2797  ynat-v1_train_02797   텔레그램 한  등 亞서  시간 다운… 버 정    종  보       4   \n",
       "2798  ynat-v1_train_02798    인터뷰 류현진 친구에게 안타 맞는 것 싫어해…승부는 냉정       1   \n",
       "2799  ynat-v1_train_02799               지능정보사회 대비 국가 종합대책 마련       4   \n",
       "\n",
       "      ascii_ratio  is_noisy  \n",
       "0        0.625000         1  \n",
       "1        0.740741         1  \n",
       "2        0.454545         1  \n",
       "3        0.310345         0  \n",
       "4        0.500000         1  \n",
       "...           ...       ...  \n",
       "2795     0.272727         0  \n",
       "2796     0.333333         0  \n",
       "2797     0.468750         1  \n",
       "2798     0.225806         0  \n",
       "2799     0.200000         0  \n",
       "\n",
       "[2800 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "1it [00:18, 18.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin :  정   파  미사      이용기간    단   분종  보\n",
      "cleaned_text :  정파 미사용 기간 단분 종보\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:34, 17.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin :   찰 국     로  한   회장       송 \n",
      "cleaned_text :  찰스 국로 한 회장 송\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "선정된 모델을 이용해 기존의 noise dataset의 텍스트 중 노이즈로 판별된 데이터만 복구하는 코드 (step-prompting)\n",
    "\"\"\"\n",
    "import re \n",
    "\n",
    "def extract_quoted_string(text):\n",
    "    pattern = r\"'([^']*)'\"  # 작은따옴표 사이의 문자열을 찾는 패턴\n",
    "    matches = re.findall(pattern, text)\n",
    "    if len(matches) == 0 :\n",
    "        return \"제목없음\"\n",
    "    else :\n",
    "        return matches[0]\n",
    "\n",
    "for idx, df in enumerate(df_list) : \n",
    "    temp_df = df.copy()\n",
    "    for tidx, (noisy_text, is_noisy) in tqdm(enumerate(zip(df['text'], df['is_noisy']))) :\n",
    "        if tidx ==2 : break\n",
    "        if is_noisy == 0 :\n",
    "          continue\n",
    "        prompt = f\"\"\"당신은 전문적인 한국어 텍스트 정제 AI입니다. 다음은 노이즈가 공백으로 치환된 한국어 뉴스 기사의 제목입니다. 이 제목에서 원래의 자연스러운 뉴스 제목으로 복원해주세요.\n",
    "\n",
    "        노이즈가 포함된 제목: '{noisy_text}'\n",
    "\n",
    "        복원 시 다음 지침을 따라주세요:\n",
    "        1. 이 문장에서 의미를 자연스럽게 만들기 위해 공백이 들어가야 할 위치를 분석하세요.\n",
    "        2. 이 문장의 공백을자연스럽게 채울 수 있는 한국어 단어를 3~5개 정도 제안해 주세요.\n",
    "        3. 이 문장의 공백에 [단어]를 넣어 자연스러운 문장으로 완성해 주세요.\n",
    "        4. 완성된 문장이 자연스러운지 검토하고, 더 적절한 단어로 수정할 부분이 있다면 수정해 주세요.\n",
    "\n",
    "        복원된 제목:\"\"\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                top_p=0.95,\n",
    "                do_sample=True\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        cleaned_text = extract_quoted_string(generated_text.split(\"복원된 제목:\")[-1].strip())\n",
    "        temp_df.loc[tidx, 'text'] = cleaned_text\n",
    "        print(\"origin : \", noisy_text)\n",
    "        print(\"cleaned_text : \", cleaned_text)\n",
    "\n",
    "    temp_df.to_csv(f'../data/recovery_data/c1_label_{idx}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "1it [00:17, 17.72s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin :  정^ ^파^ 미사^ ^^^ 이용기간 ^^ 단^ ^분종^^보\n",
      "cleaned_text :  정파 미사일 ^^^ 이용기간 ^ 단 ^ 분종 ^보\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:35, 17.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin :  ^찰^국^^^^ 로^^한^^ 회장 ^ ^^^^송^\n",
      "cleaned_text :  찰스 국 로한 회장 승송\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:52, 17.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin :  ^ 김정^ 자주통일 새^^^열^나가야^보\n",
      "cleaned_text :  김정은 자주통일 새 시대 열어 나가야 1보 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "선정된 모델을 이용해 기존의 noise dataset의 텍스트 중 노이즈로 판별된 데이터만 복구하는 코드 (noise는 ^로 변환환, few-shot)\n",
    "\"\"\"\n",
    "import re \n",
    "\n",
    "def extract_quoted_string(text):\n",
    "    pattern = r\"'([^']*)'\"  # 작은따옴표 사이의 문자열을 찾는 패턴\n",
    "    matches = re.findall(pattern, text)\n",
    "    if len(matches) == 0 :\n",
    "        return \"제목없음\"\n",
    "    else :\n",
    "        return matches[0]\n",
    "\n",
    "for idx, df in enumerate(df_list) : \n",
    "    temp_df = df.copy()\n",
    "    for tidx, (noisy_text, is_noisy) in tqdm(enumerate(zip(df['text'], df['is_noisy']))) :\n",
    "        if tidx == 20 : break\n",
    "        if is_noisy == 0 :\n",
    "          continue\n",
    "        prompt = f\"\"\"당신은 전문적인 한국어 텍스트 복원 AI입니다. 다음은 노이즈가 \"^\"로 치환된 한국어 뉴스 기사의 제목입니다. 이 제목에서 원래의 자연스러운 뉴스 제목으로 복원해주세요.\n",
    "\n",
    "\n",
    "        다음은 노이즈를 원래의 문자로 복구하여 자연스러운 문장을 생성한 예시입니다:\n",
    "\n",
    "        1. 노이즈가 포함된 제목: '^^^레콤 ^분기 영^익^^천^^^^…^^^％ 증가'\n",
    "        복원된 제목: 'SK텔레콤 4분기 영업익 오천오백육억...200% 증가'\n",
    "\n",
    "        2. 노이즈가 포함된 제목: '감사원 수^과학^구소^연구보다^워크^·강연'\n",
    "        복원된 제목: '감사원 수리과학연구소 연구보다 워크샵·강연'\n",
    "\n",
    "        3. 노이즈가 포함된 제목: '^^∼^^대는 유튜브 세대…하루 ^^^회 ^^분 본다'\n",
    "        복원된 제목: '10∼20대는 유튜브 세대…하루 100회 90분 본다'\n",
    "\n",
    "        4. 노이즈가 포함된 제목: '^ 김정^ 자주통일 새^^^열^나가야^보'\n",
    "        복원된 제목: '北 김정은 자주통일 새 시대 열어 나가야 1보 '\n",
    "        \n",
    "        위의 예시와 같이 복원할 때에는 다음과 같은 규칙이 존재합니다.\n",
    "        1. ^ 문자가 한 번만 등장하면 그 자리에 적합한 한 글자를 넣어 자연스럽게 복원합니다.\n",
    "        2. ^ 문자가 여러 번 연속으로 등장하면, ^의 개수를 유지하면서 그 자리에 필요한 단어를 넣습니다.\n",
    "        \n",
    "        이제 다음 제목의 노이즈를 문장에 어울리는 단어로 치환하여 자연스러운 문장으로 만들어주세요:\n",
    "\n",
    "        노이즈가 포함된 제목: '{noisy_text}'\n",
    "\n",
    "        복원된 제목:\"\"\"\n",
    "        ##         1. \"^\"와 공백이 아닌 기존 한글 문자는 그대로 둔 채 최대한 나머지 문자를 채워주세요.\n",
    "        ##         2. \"^\"는 무조건 \"^\"와 공백이 아닌 한 글자로 치환해주세요\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                top_p=0.95,\n",
    "                do_sample=True\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        cleaned_text = extract_quoted_string(generated_text.split(\"복원된 제목:\")[-1].strip())\n",
    "        temp_df.loc[tidx, 'text'] = cleaned_text\n",
    "        # print(\"generated_text : \", generated_text)\n",
    "        print(\"origin : \", noisy_text)\n",
    "        print(\"cleaned_text : \", cleaned_text)\n",
    "\n",
    "    temp_df.to_csv(f'../data/recovery_data/converted_train_ver2_fewshot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "선정된 모델을 이용해 기존의 noise dataset의 텍스트 중 노이즈로 판별된 데이터만 복구하는 코드 (noise는 ^로 변환환)\n",
    "\"\"\"\n",
    "import re \n",
    "\n",
    "def extract_quoted_string(text):\n",
    "    pattern = r\"'([^']*)'\"  # 작은따옴표 사이의 문자열을 찾는 패턴\n",
    "    matches = re.findall(pattern, text)\n",
    "    if len(matches) == 0 :\n",
    "        return \"제목없음\"\n",
    "    else :\n",
    "        return matches[0]\n",
    "\n",
    "for idx, df in enumerate(df_list) : \n",
    "    temp_df = df.copy()\n",
    "    for tidx, (noisy_text, is_noisy) in tqdm(enumerate(zip(df['text'], df['is_noisy']))) :\n",
    "        if tidx ==2 : break\n",
    "        if is_noisy == 0 :\n",
    "          continue\n",
    "        prompt = f\"\"\"당신은 전문적인 한국어 텍스트 정제 AI입니다. 다음은 노이즈가 공백으로 치환된 한국어 뉴스 기사의 제목입니다. 이 제목에서 원래의 자연스러운 뉴스 제목으로 복원해주세요.\n",
    "\n",
    "        노이즈가 포함된 제목: '{noisy_text}'\n",
    "\n",
    "        복원 시 다음 지침을 따라주세요:\n",
    "        1. 이 문장에서 의미를 자연스럽게 만들기 위해 공백이 들어가야 할 위치를 분석하세요.\n",
    "        2. 이 문장의 공백을자연스럽게 채울 수 있는 한국어 단어를 3~5개 정도 제안해 주세요.\n",
    "        3. 이 문장의 공백에 [단어]를 넣어 자연스러운 문장으로 완성해 주세요.\n",
    "        4. 완성된 문장이 자연스러운지 검토하고, 더 적절한 단어로 수정할 부분이 있다면 수정해 주세요.\n",
    "\n",
    "        복원된 제목:\"\"\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                top_p=0.95,\n",
    "                do_sample=True\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        cleaned_text = extract_quoted_string(generated_text.split(\"복원된 제목:\")[-1].strip())\n",
    "        temp_df.loc[tidx, 'text'] = cleaned_text\n",
    "        print(\"origin : \", noisy_text)\n",
    "        print(\"cleaned_text : \", cleaned_text)\n",
    "\n",
    "    temp_df.to_csv(f'../data/recovery_data/c1_label_{idx}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "선정된 모델을 이용해 기존의 noise dataset을 처리하는 코드 2 \n",
    "\"\"\"\n",
    "import re\n",
    "\n",
    "def extract_quoted_string(text):\n",
    "    pattern = r\"'([^']*)'\"  # 작은따옴표 사이의 문자열을 찾는 패턴\n",
    "    matches = re.findall(pattern, text)\n",
    "    if len(matches) == 0 :\n",
    "        return \"제목없음\"\n",
    "    else :\n",
    "        return matches[0]\n",
    "\n",
    "\n",
    "for idx, df in enumerate(df_list) : \n",
    "    temp_df = df.copy()\n",
    "    for tidx, noisy_text in tqdm(enumerate(df['text'])) :\n",
    "        prompt = f\"\"\"당신은 전문적인 한국어 텍스트 정제 AI입니다. 다음은 노이즈가 포함된 한국어 뉴스 기사의 제목입니다. 이 제목에서 노이즈를 제거하고 원래의 자연스러운 뉴스 제목으로 복원해주세요.\n",
    "\n",
    "        노이즈가 포함된 제목: '{noisy_text}'\n",
    "\n",
    "        복원 시 다음 지침을 따라주세요:\n",
    "        1. 무작위로 삽입된 영문자와 숫자를 제거하세요.\n",
    "        2. 특수문자를 적절히 처리하세요.\n",
    "        3. 줄임말이나 약어는 가능한 원래 형태로 복원하세요.\n",
    "        4. 문맥을 고려하여 누락된 단어나 조사를 추가하세요.\n",
    "        5. 제목의 전체적인 의미를 유지하면서 자연스러운 한국어 문장으로 만드세요.\n",
    "\n",
    "        복원된 제목:\"\"\"\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                top_p=0.95,\n",
    "                do_sample=True\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        cleaned_text = extract_quoted_string(generated_text.split(\"복원된 제목:\")[-1].strip())\n",
    "        temp_df.loc[tidx, 'text'] = cleaned_text\n",
    "        print(\"origin : \", noisy_text)\n",
    "        print(\"cleaned : \", cleaned_text)\n",
    "\n",
    "    temp_df.to_csv(f'./data/cleaning_step1/c1_label_{idx}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_venv",
   "language": "python",
   "name": "py_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
